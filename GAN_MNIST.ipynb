{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\n",
    "x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1, 784])\n",
    "x_train, x_test = x_train / 255., x_test / 255.\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(8, 8)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        plt.imshow(sample.reshape(28, 28))\n",
    "\n",
    "    return fig\n",
    "\n",
    "num_epoch = 100000\n",
    "batch_size = 64\n",
    "num_input = 28 * 28\n",
    "num_latent_variable = 100   # 잠재 변수 z의 차원\n",
    "num_hidden = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_data = train_data.repeat().shuffle(60000).batch(batch_size)\n",
    "train_data_iter = iter(train_data)\n",
    "\n",
    "class Generator(object):\n",
    "    def __init__(self):\n",
    "        self.G_W1 = tf.Variable(tf.random.normal(shape=[num_latent_variable, num_hidden], stddev=5e-2))\n",
    "        self.G_b1 = tf.Variable(tf.constant(0.1, shape=[num_hidden]))\n",
    "        self.G_W2 = tf.Variable(tf.random.normal(shape=[num_hidden, num_input], stddev=5e-2))\n",
    "        self.G_b2 = tf.Variable(tf.constant(0.1, shape=[num_input]))\n",
    "\n",
    "    def __call__(self, X):\n",
    "        hidden_layer = tf.nn.relu((tf.matmul(X, self.G_W1) + self.G_b1))\n",
    "        output_layer = tf.matmul(hidden_layer, self.G_W2) + self.G_b2\n",
    "        generated_mnist_image = tf.nn.sigmoid(output_layer)\n",
    "\n",
    "        return generated_mnist_image\n",
    "\n",
    "class Discriminator(object):\n",
    "    def __init__(self):\n",
    "        self.D_W1 = tf.Variable(tf.random.normal(shape=[num_input, num_hidden], stddev=5e-2))\n",
    "        self.D_b1 = tf.Variable(tf.constant(0.1, shape=[num_hidden]))\n",
    "        self.D_W2 = tf.Variable(tf.random.normal(shape=[num_hidden, 1], stddev=5e-2))\n",
    "        self.D_b2 = tf.Variable(tf.constant(0.1, shape=[1]))\n",
    "\n",
    "    def __call__(self, X):\n",
    "        hidden_layer = tf.nn.relu((tf.matmul(X, self.D_W1) + self.D_b1))\n",
    "        logits = tf.matmul(hidden_layer, self.D_W2) + self.D_b2\n",
    "        predicted_value = tf.nn.sigmoid(logits)\n",
    "\n",
    "        return predicted_value, logits\n",
    "\n",
    "@tf.function\n",
    "def generator_loss(D_fake_logits):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake_logits)))         # log(D(G(z))\n",
    "\n",
    "@tf.function\n",
    "def discriminator_loss(D_real_logits, D_fake_logits):\n",
    "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real_logits)))  # log(D(x))\n",
    "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake_logits)))  # log(1-D(G(z)))\n",
    "    d_loss = d_loss_real + d_loss_fake  # log(D(x)) + log(1-D(G(z)))\n",
    "\n",
    "    return d_loss\n",
    "\n",
    "Generator_model = Generator()\n",
    "\n",
    "Discriminator_model = Discriminator()\n",
    "\n",
    "discriminator_optimizer = tf.optimizers.Adam(learning_rate)\n",
    "generator_optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "@tf.function\n",
    "def d_train_step(discriminator_model, real_image, fake_image):\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        D_real, D_real_logits = discriminator_model(real_image)  # D(x)\n",
    "        D_fake, D_fake_logits = discriminator_model(fake_image)  # D(G(z))\n",
    "        loss = discriminator_loss(D_real_logits, D_fake_logits)\n",
    "    gradients = disc_tape.gradient(loss, vars(discriminator_model).values())\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients, vars(discriminator_model).values()))\n",
    "\n",
    "@tf.function\n",
    "def g_train_step(generator_model, discriminator_model, z):\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        G = generator_model(z)\n",
    "        D_fake, D_fake_logits = discriminator_model(G)  # D(G(z))\n",
    "        loss = generator_loss(D_fake_logits)\n",
    "    gradients = gen_tape.gradient(loss, vars(generator_model).values())\n",
    "    generator_optimizer.apply_gradients(zip(gradients, vars(generator_model).values()))\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists('generated_output/'):\n",
    "    os.makedirs('generated_output/')\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    batch_X = next(train_data_iter)\n",
    "\n",
    "    batch_noise = np.random.uniform(-1., 1., [batch_size, 100]).astype('float32')\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        samples = Generator_model(np.random.uniform(-1., 1., [64, 100]).astype('float32')).numpy()\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('generated_output/%s.png' % str(num_img).zfill(3), bbox_inches='tight')\n",
    "        num_img += 1\n",
    "        plt.close(fig)\n",
    "\n",
    "    _, d_loss_print = d_train_step(Discriminator_model, batch_X, Generator_model(batch_noise)), discriminator_loss(Discriminator_model(batch_X)[1], Discriminator_model(Generator_model(batch_noise))[1])\n",
    "\n",
    "    _, g_loss_print = g_train_step(Generator_model, Discriminator_model, batch_noise), generator_loss(Discriminator_model(Generator_model(batch_noise))[1])\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('반복(Epoch): %d, Generator 손실함수(g_loss): %f, Discriminator 손실함수(d_loss): %f' % (i, g_loss_print, d_loss_print))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

